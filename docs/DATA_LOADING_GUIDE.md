# ðŸ“š Data Loading with Error Handling - Complete Guide

## âœ… New Robust Data Loader

I've created a comprehensive data loading utility with **full error handling** for all data fetching operations.

---

## ðŸŽ¯ Features

### âœ… Comprehensive Error Handling
- File not found errors
- Empty file detection
- File size validation (max 100MB)
- Encoding issues (tries multiple encodings)
- Parser errors
- Memory errors
- Validation errors

### âœ… Multiple File Formats
- CSV (`.csv`)
- Excel (`.xlsx`, `.xls`)
- JSON (`.json`)
- Parquet (`.parquet`)

### âœ… Automatic Fixes
- Column name normalization (spaces â†’ underscores)
- Duplicate column handling
- Invalid character removal
- Column name deduplication

### âœ… Data Validation
- Empty DataFrame detection
- Minimum row requirements
- All-NaN detection
- Duplicate column warnings
- Header validation

---

## ðŸ“– Usage Examples

### Example 1: Load CSV File

```python
from src.utils import DataLoader

loader = DataLoader()

# Load CSV with full error handling
df, error = loader.load_csv('data/campaign_data.csv')

if error:
    print(f"âŒ Error: {error}")
else:
    print(f"âœ… Loaded {len(df)} rows successfully!")
```

### Example 2: Load from Streamlit Upload

```python
import streamlit as st
from src.utils import DataLoader

uploaded_file = st.file_uploader("Upload CSV", type=['csv'])

if uploaded_file:
    df, error = DataLoader.load_from_streamlit_upload(uploaded_file)
    
    if error:
        st.error(f"âŒ {error}")
    else:
        st.success(f"âœ… Loaded {len(df):,} rows!")
        st.dataframe(df.head())
```

### Example 3: Simple Load (Returns None on Error)

```python
from src.utils import safe_load_csv

# Returns DataFrame or None
df = safe_load_csv('data/campaigns.csv')

if df is not None:
    print("Data loaded successfully!")
else:
    print("Failed to load data")
```

### Example 4: Generic File Loader

```python
from src.utils import fetch_data

# Automatically detects file type
df, error = fetch_data('data/campaigns.csv')  # CSV
df, error = fetch_data('data/campaigns.xlsx')  # Excel
df, error = fetch_data('data/campaigns.json')  # JSON
df, error = fetch_data('data/campaigns.parquet')  # Parquet

if error:
    print(f"Error: {error}")
```

---

## ðŸ”§ Advanced Options

### Disable Validation

```python
df, error = loader.load_csv(
    'data/campaigns.csv',
    validate=False  # Skip validation
)
```

### Keep Original Column Names

```python
df, error = loader.load_csv(
    'data/campaigns.csv',
    fix_column_names=False  # Don't modify column names
)
```

### Custom Encoding

```python
df, error = loader.load_csv(
    'data/campaigns.csv',
    encoding='latin-1'  # Specify encoding
)
```

### Load Specific Excel Sheet

```python
df, error = loader.load_excel(
    'data/campaigns.xlsx',
    sheet_name='Sheet2'  # or sheet index: 1
)
```

---

## ðŸš¨ Error Messages

The loader provides clear, actionable error messages:

| Error | Message | Solution |
|-------|---------|----------|
| File not found | `File not found: path/to/file.csv` | Check file path |
| Empty file | `File is empty: path/to/file.csv` | Ensure file has data |
| Too large | `File too large (150MB). Maximum: 100MB` | Split file or increase limit |
| Wrong format | `Unsupported file type: .txt` | Use CSV, Excel, JSON, or Parquet |
| No data | `DataFrame is empty (0 rows)` | Check file content |
| Encoding | `Failed to read CSV: codec can't decode` | Try different encoding |
| Parser error | `CSV parsing error: ...` | Check CSV format |
| Memory | `Not enough memory to load this file` | Use smaller file |

---

## ðŸŽ¨ Integration Examples

### In Streamlit App

```python
import streamlit as st
from src.utils import DataLoader

st.title("Campaign Data Upload")

uploaded_file = st.file_uploader(
    "Upload Campaign Data",
    type=['csv', 'xlsx', 'json'],
    help="Upload your campaign performance data"
)

if uploaded_file:
    with st.spinner("Loading data..."):
        df, error = DataLoader.load_from_streamlit_upload(
            uploaded_file,
            validate=True,
            fix_column_names=True
        )
    
    if error:
        st.error(f"âŒ Failed to load data: {error}")
        st.info("ðŸ’¡ Please check your file and try again")
    else:
        st.success(f"âœ… Successfully loaded {len(df):,} rows with {len(df.columns)} columns")
        
        # Show data preview
        with st.expander("ðŸ“Š Data Preview"):
            st.dataframe(df.head(10))
        
        # Show column info
        with st.expander("ðŸ“‹ Column Information"):
            st.write(f"**Columns ({len(df.columns)}):**")
            st.write(df.columns.tolist())
```

### In Q&A Engine

```python
from src.query_engine import NaturalLanguageQueryEngine
from src.utils import DataLoader

# Load data with error handling
df, error = DataLoader.load_csv('data/campaigns.csv')

if error:
    print(f"Error loading data: {error}")
    exit(1)

# Initialize Q&A engine
engine = NaturalLanguageQueryEngine(api_key="your-key")
engine.load_data(df)

# Ask questions
result = engine.ask("What is the total spend?")
print(result['answer'])
```

### In Automated Analysis

```python
from src.analytics import MediaAnalyticsExpert
from src.utils import DataLoader

# Load data safely
df, error = DataLoader.load_csv('data/campaigns.csv')

if error:
    print(f"Cannot run analysis: {error}")
    exit(1)

# Run analysis
expert = MediaAnalyticsExpert(api_key="your-key")
analysis = expert.analyze_all(df)

print(analysis['executive_summary'])
```

---

## ðŸ“ Configuration

### Adjust Maximum File Size

```python
from src.utils import DataLoader

# Increase to 200MB
DataLoader.MAX_FILE_SIZE = 200 * 1024 * 1024

df, error = DataLoader.load_csv('large_file.csv')
```

### Adjust Minimum Rows

```python
from src.utils import DataLoader

# Require at least 100 rows
DataLoader.MIN_ROWS = 100

df, error = DataLoader.load_csv('data.csv')
```

---

## ðŸ” Validation Details

The loader validates:

1. **File Existence** - File must exist
2. **File Size** - Must be > 0 and < MAX_FILE_SIZE
3. **File Extension** - Must be supported format
4. **Data Content** - Must have at least MIN_ROWS rows
5. **Column Names** - Warns about duplicates
6. **Data Values** - Checks for all-NaN data

---

## ðŸŽ¯ Best Practices

### 1. Always Check for Errors

```python
# âŒ Bad
df, _ = DataLoader.load_csv('data.csv')
# Ignores errors!

# âœ… Good
df, error = DataLoader.load_csv('data.csv')
if error:
    handle_error(error)
```

### 2. Use Try-Except for Additional Safety

```python
try:
    df, error = DataLoader.load_csv('data.csv')
    if error:
        logger.error(f"Load failed: {error}")
        return
    
    # Process data
    process_data(df)
    
except Exception as e:
    logger.error(f"Unexpected error: {e}")
```

### 3. Provide User Feedback

```python
# In Streamlit
with st.spinner("Loading data..."):
    df, error = DataLoader.load_csv(file_path)

if error:
    st.error(f"âŒ {error}")
    st.info("ðŸ’¡ Tip: Check file format and try again")
else:
    st.success("âœ… Data loaded successfully!")
```

---

## ðŸ“Š Performance

- **Small files (<1MB)**: < 1 second
- **Medium files (1-10MB)**: 1-5 seconds
- **Large files (10-100MB)**: 5-30 seconds

The loader tries multiple encodings if needed, which may add 1-2 seconds for problematic files.

---

## ðŸ†˜ Troubleshooting

### Issue: "File not found"
**Solution:** Check the file path is correct and file exists

### Issue: "Encoding error"
**Solution:** The loader tries multiple encodings automatically. If it still fails, try:
```python
df, error = DataLoader.load_csv('data.csv', encoding='latin-1')
```

### Issue: "File too large"
**Solution:** Either:
1. Split the file into smaller chunks
2. Increase MAX_FILE_SIZE
3. Use Parquet format (more efficient)

### Issue: "All columns are unnamed"
**Solution:** Ensure your CSV has a header row

---

## ðŸ“š API Reference

### DataLoader Class

```python
class DataLoader:
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    MIN_ROWS = 1
    SUPPORTED_EXTENSIONS = ['.csv', '.xlsx', '.xls', '.json', '.parquet']
    
    @staticmethod
    def load_csv(file_path, validate=True, fix_column_names=True, encoding='utf-8')
    
    @staticmethod
    def load_excel(file_path, validate=True, fix_column_names=True, sheet_name=0)
    
    @staticmethod
    def load_json(file_path, validate=True, fix_column_names=True)
    
    @staticmethod
    def load_parquet(file_path, validate=True, fix_column_names=True)
    
    @staticmethod
    def load_from_streamlit_upload(uploaded_file, validate=True, fix_column_names=True)
```

### Helper Functions

```python
def fetch_data(file_path, **kwargs) -> Tuple[Optional[pd.DataFrame], Optional[str]]
    # Auto-detects file type and loads

def safe_load_csv(file_path, **kwargs) -> Optional[pd.DataFrame]
    # Returns DataFrame or None (no error message)
```

---

## âœ… Summary

**The new DataLoader provides:**
- âœ… Comprehensive error handling
- âœ… Multiple file format support
- âœ… Automatic column name fixing
- âœ… Data validation
- âœ… Clear error messages
- âœ… Easy integration
- âœ… Backward compatible

**Use it everywhere you load data for robust, production-ready code!**

---

**File Location:** `src/utils/data_loader.py`

**Import:**
```python
from src.utils import DataLoader, fetch_data, safe_load_csv
```

**Ready to use!** ðŸš€
